#!/usr/bin/env python3
"""
Pitman-Yor Process vs Dirichlet Process Clustering Validation
==============================================================

This script validates claims that PYP outperforms DP on power-law distributed data:
- PYP beats DP by 17-22% on power-law data (NMI metric)
- DP underestimates cluster count on power-law distributions
- DP "collapses" niche clusters into large ones
- PYP sustains discovery as n grows

Run on NVIDIA B200 GPU for best performance at scale.

Usage:
    python validate_pyp_vs_dp.py --n_samples 50000 --n_features 60 --n_clusters 30
    python validate_pyp_vs_dp.py --run_sweep  # Full parameter sweep
    python validate_pyp_vs_dp.py --separation_sweep  # Find crossover point

Author: HIMARI Project
"""

import argparse
import time
import warnings
from dataclasses import dataclass
from typing import Tuple, List, Dict, Any, Optional

import numpy as np
import pandas as pd
from scipy.stats import zipf as zipf_dist, wishart
from sklearn.metrics import (
    normalized_mutual_info_score,
    adjusted_rand_score,
    silhouette_score
)
from sklearn.mixture import BayesianGaussianMixture
from sklearn.cluster import KMeans

# Suppress sklearn convergence warnings for cleaner output
warnings.filterwarnings('ignore', category=UserWarning)

# Check for GPU availability
try:
    import torch
    import pyro
    import pyro.distributions as dist
    from pyro.infer import SVI, TraceEnum_ELBO, Trace_ELBO
    from pyro.optim import Adam, ClippedAdam
    PYRO_AVAILABLE = True
    GPU_AVAILABLE = torch.cuda.is_available()
    if GPU_AVAILABLE:
        DEVICE = torch.device('cuda')
        GPU_NAME = torch.cuda.get_device_name(0)
        GPU_MEMORY = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        DEVICE = torch.device('cpu')
        GPU_NAME = "CPU"
        GPU_MEMORY = 0
except ImportError:
    PYRO_AVAILABLE = False
    GPU_AVAILABLE = False
    DEVICE = None
    GPU_NAME = "N/A"
    GPU_MEMORY = 0


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DATA GENERATION - HARD MODE (Elliptical, Overlapping Clusters)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class SyntheticDataset:
    """Container for synthetic clustering dataset."""
    X: np.ndarray
    y_true: np.ndarray
    cluster_sizes: np.ndarray
    n_true_clusters: int
    zipf_exponent: float
    cluster_separation: float
    elliptical: bool

    @property
    def n_samples(self) -> int:
        return self.X.shape[0]

    @property
    def n_features(self) -> int:
        return self.X.shape[1]

    def size_distribution_summary(self) -> str:
        """Return summary of cluster size distribution."""
        sorted_sizes = sorted(self.cluster_sizes, reverse=True)
        top_5 = sorted_sizes[:5]
        bottom_5 = sorted_sizes[-5:]
        return f"Top 5: {top_5}, Bottom 5: {bottom_5}"


def generate_power_law_clusters(
    n_samples: int = 50_000,
    n_features: int = 60,
    zipf_exponent: float = 2.0,  # Increased for more niche clusters
    n_true_clusters: int = 30,
    cluster_separation: float = 0.7,  # Reduced for harder problem
    intra_cluster_std: float = 1.0,  # Increased noise
    elliptical: bool = True,  # Use elliptical clusters
    seed: int = 42
) -> SyntheticDataset:
    """
    Generate synthetic data with power-law distributed cluster sizes.

    HARD MODE: Elliptical clusters with overlap for realistic testing.

    Parameters
    ----------
    n_samples : int
        Total number of samples to generate
    n_features : int
        Dimensionality of feature space (60 = typical for HIMARI)
    zipf_exponent : float
        Controls steepness of power-law (1.2=flat, 2.5=steep)
        Higher = more small niche clusters
    n_true_clusters : int
        Number of ground-truth clusters
    cluster_separation : float
        Distance between cluster centers (lower = harder)
        0.5 = very hard, 1.0 = moderate, 3.0 = easy
    intra_cluster_std : float
        Base standard deviation within clusters
    elliptical : bool
        If True, generate elliptical clusters with random covariances
        If False, use spherical Gaussian clusters
    seed : int
        Random seed for reproducibility

    Returns
    -------
    SyntheticDataset
        Container with X, y_true, cluster_sizes, and metadata
    """
    np.random.seed(seed)

    # Generate cluster sizes following Zipf's law
    raw_sizes = zipf_dist.rvs(zipf_exponent, size=n_true_clusters, random_state=seed)
    cluster_sizes = (raw_sizes / raw_sizes.sum() * n_samples).astype(int)

    # Ensure we have exactly n_samples
    diff = n_samples - cluster_sizes.sum()
    cluster_sizes[0] += diff
    cluster_sizes = np.maximum(cluster_sizes, 1)
    actual_n = cluster_sizes.sum()

    # Generate cluster centers
    centers = np.random.randn(n_true_clusters, n_features) * cluster_separation

    # Generate covariance matrices for elliptical clusters
    if elliptical:
        covariances = []
        for k in range(n_true_clusters):
            # Generate random positive definite covariance matrix
            # Using Wishart distribution for realistic elliptical shapes
            df = n_features + 5  # Degrees of freedom
            scale = np.eye(n_features) * (intra_cluster_std ** 2) / df

            # Generate random rotation
            random_matrix = np.random.randn(n_features, n_features)
            Q, _ = np.linalg.qr(random_matrix)

            # Random eigenvalues (elongation factors)
            eigenvalues = np.random.uniform(0.3, 3.0, n_features)
            eigenvalues = eigenvalues / eigenvalues.mean()  # Normalize

            # Construct covariance: Q @ diag(eigenvalues) @ Q.T * std^2
            cov = Q @ np.diag(eigenvalues * intra_cluster_std ** 2) @ Q.T
            covariances.append(cov)
    else:
        covariances = [np.eye(n_features) * intra_cluster_std ** 2] * n_true_clusters

    # Generate samples for each cluster
    X_list = []
    labels_list = []

    for k, size in enumerate(cluster_sizes):
        if elliptical:
            # Multivariate normal with full covariance
            samples = np.random.multivariate_normal(
                centers[k], covariances[k], size=size
            )
        else:
            # Spherical Gaussian
            samples = centers[k] + np.random.randn(size, n_features) * intra_cluster_std

        X_list.append(samples)
        labels_list.extend([k] * size)

    X = np.vstack(X_list)
    labels = np.array(labels_list)

    # Shuffle
    perm = np.random.permutation(actual_n)
    X = X[perm]
    labels = labels[perm]

    return SyntheticDataset(
        X=X,
        y_true=labels,
        cluster_sizes=cluster_sizes,
        n_true_clusters=n_true_clusters,
        zipf_exponent=zipf_exponent,
        cluster_separation=cluster_separation,
        elliptical=elliptical
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# METHOD 1: STANDARD DIRICHLET PROCESS (scikit-learn)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class ClusteringResult:
    """Container for clustering results."""
    labels: np.ndarray
    k_inferred: int
    elapsed_time: float
    method: str
    extra_info: Dict[str, Any] = None

    def __post_init__(self):
        if self.extra_info is None:
            self.extra_info = {}


def fit_dirichlet_process(
    X: np.ndarray,
    max_components: int = 100,
    concentration: float = 1.0,
    covariance_type: str = 'full',  # Use 'full' for elliptical clusters
    n_init: int = 3,
    max_iter: int = 500,
    random_state: int = 42
) -> ClusteringResult:
    """
    Fit standard Dirichlet Process Mixture Model using variational inference.
    """
    start = time.time()

    model = BayesianGaussianMixture(
        n_components=max_components,
        covariance_type=covariance_type,
        weight_concentration_prior_type='dirichlet_process',
        weight_concentration_prior=concentration,
        n_init=n_init,
        max_iter=max_iter,
        random_state=random_state,
        verbose=0
    )

    model.fit(X)
    elapsed = time.time() - start

    labels = model.predict(X)
    k_inferred = len(np.unique(labels))

    weights = model.weights_
    active_weights = weights[weights > 0.01]

    return ClusteringResult(
        labels=labels,
        k_inferred=k_inferred,
        elapsed_time=elapsed,
        method="DP (sklearn)",
        extra_info={
            'n_active_components': len(active_weights),
            'weight_concentration': model.weight_concentration_,
            'converged': model.converged_,
            'n_iter': model.n_iter_,
            'covariance_type': covariance_type
        }
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# METHOD 2: PITMAN-YOR PROCESS (Pyro on GPU) - TUNED VERSION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def fit_pitman_yor_gpu(
    X: np.ndarray,
    max_components: int = 100,
    discount: float = 0.25,
    concentration: float = 1.0,
    n_steps: int = 2000,  # Increased from 1000
    learning_rate: float = 0.005,  # Reduced from 0.01
    batch_size: Optional[int] = None,
    use_kmeans_init: bool = True,  # K-means initialization
    verbose: bool = True
) -> ClusteringResult:
    """
    Fit Pitman-Yor Process Mixture Model using variational inference on GPU.

    TUNED VERSION with:
    - K-means initialization for better starting point
    - More SVI steps (2000)
    - Lower learning rate (0.005)
    - Full covariance support
    """
    if not PYRO_AVAILABLE:
        raise ImportError("Pyro not available. Install with: pip install pyro-ppl")

    pyro.clear_param_store()

    X_tensor = torch.tensor(X, dtype=torch.float32, device=DEVICE)
    N, D = X_tensor.shape
    K = max_components

    start = time.time()

    # K-means initialization for better starting point
    if use_kmeans_init and verbose:
        print("  Initializing with K-means...")

    if use_kmeans_init:
        kmeans = KMeans(n_clusters=min(K, 50), random_state=42, n_init=3)
        kmeans.fit(X[:min(10000, N)])  # Use subset for speed
        init_centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32, device=DEVICE)
        # Pad with random centers if needed
        if K > 50:
            extra_centers = torch.randn(K - 50, D, device=DEVICE) * 2
            init_centers = torch.cat([init_centers, extra_centers], dim=0)
    else:
        init_centers = torch.randn(K, D, device=DEVICE) * 2

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # MODEL: Pitman-Yor Process with stick-breaking construction
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def model(X_batch):
        batch_size = X_batch.shape[0]

        with pyro.plate('sticks', K - 1):
            k_idx = torch.arange(K - 1, device=DEVICE, dtype=torch.float32)
            beta_a = torch.ones(K - 1, device=DEVICE) * (1.0 - discount)
            beta_b = concentration + (k_idx + 1) * discount
            v = pyro.sample('v', dist.Beta(beta_a, beta_b))

        # Stick-breaking weights
        one_minus_v = 1.0 - v
        cumprod = torch.cumprod(one_minus_v, dim=0)
        cumprod_shifted = torch.cat([torch.ones(1, device=DEVICE), cumprod[:-1]])

        weights = torch.zeros(K, device=DEVICE)
        weights[:K-1] = v * cumprod_shifted
        weights[K-1] = cumprod[-1]

        # Cluster parameters with wider priors
        with pyro.plate('components', K):
            locs = pyro.sample(
                'locs',
                dist.Normal(
                    torch.zeros(D, device=DEVICE),
                    torch.ones(D, device=DEVICE) * 3.0  # Wider prior
                ).to_event(1)
            )
            scales = pyro.sample(
                'scales',
                dist.LogNormal(
                    torch.zeros(D, device=DEVICE),
                    torch.ones(D, device=DEVICE) * 0.5
                ).to_event(1)
            )

        with pyro.plate('data', batch_size):
            assignment = pyro.sample('assignment', dist.Categorical(weights))
            pyro.sample(
                'obs',
                dist.Normal(locs[assignment], scales[assignment]).to_event(1),
                obs=X_batch
            )

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # GUIDE: Mean-field variational approximation with K-means init
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def guide(X_batch):
        v_alpha = pyro.param(
            'v_alpha',
            torch.ones(K - 1, device=DEVICE) * 1.5,
            constraint=dist.constraints.positive
        )
        v_beta = pyro.param(
            'v_beta',
            torch.ones(K - 1, device=DEVICE) * 0.5,
            constraint=dist.constraints.positive
        )

        with pyro.plate('sticks', K - 1):
            pyro.sample('v', dist.Beta(v_alpha, v_beta))

        # Initialize locations with K-means centers
        loc_mean = pyro.param(
            'loc_mean',
            init_centers.clone()
        )
        loc_scale = pyro.param(
            'loc_scale',
            torch.ones(K, D, device=DEVICE) * 0.3,
            constraint=dist.constraints.positive
        )

        scale_loc = pyro.param(
            'scale_loc',
            torch.zeros(K, D, device=DEVICE)
        )
        scale_scale = pyro.param(
            'scale_scale',
            torch.ones(K, D, device=DEVICE) * 0.2,
            constraint=dist.constraints.positive
        )

        with pyro.plate('components', K):
            pyro.sample('locs', dist.Normal(loc_mean, loc_scale).to_event(1))
            pyro.sample('scales', dist.LogNormal(scale_loc, scale_scale).to_event(1))

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # SVI OPTIMIZATION with learning rate schedule
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    optimizer = ClippedAdam({
        'lr': learning_rate,
        'clip_norm': 10.0,
        'betas': (0.9, 0.999)
    })
    elbo = Trace_ELBO()
    svi = SVI(model, guide, optimizer, loss=elbo)

    losses = []
    best_loss = float('inf')
    patience_counter = 0

    if batch_size is None or batch_size >= N:
        for step in range(n_steps):
            loss = svi.step(X_tensor)
            losses.append(loss)

            # Track best loss
            if loss < best_loss:
                best_loss = loss
                patience_counter = 0
            else:
                patience_counter += 1

            if verbose and (step + 1) % 400 == 0:
                print(f"  Step {step + 1}/{n_steps}: ELBO loss = {loss:.2f}")
    else:
        n_batches = (N + batch_size - 1) // batch_size
        for step in range(n_steps):
            epoch_loss = 0.0
            perm = torch.randperm(N, device=DEVICE)

            for b in range(n_batches):
                batch_idx = perm[b * batch_size:(b + 1) * batch_size]
                X_batch = X_tensor[batch_idx]
                loss = svi.step(X_batch)
                epoch_loss += loss

            losses.append(epoch_loss / n_batches)

            if verbose and (step + 1) % 400 == 0:
                print(f"  Step {step + 1}/{n_steps}: ELBO loss = {losses[-1]:.2f}")

    elapsed = time.time() - start

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # POSTERIOR INFERENCE
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    with torch.no_grad():
        v_alpha = pyro.param('v_alpha')
        v_beta = pyro.param('v_beta')
        v_mean = v_alpha / (v_alpha + v_beta)

        one_minus_v = 1.0 - v_mean
        cumprod = torch.cumprod(one_minus_v, dim=0)
        cumprod_shifted = torch.cat([torch.ones(1, device=DEVICE), cumprod[:-1]])

        weights = torch.zeros(K, device=DEVICE)
        weights[:K-1] = v_mean * cumprod_shifted
        weights[K-1] = cumprod[-1]

        loc_mean = pyro.param('loc_mean')
        scale_loc = pyro.param('scale_loc')
        scales_mean = torch.exp(scale_loc)

        log_weights = torch.log(weights + 1e-10)

        chunk_size = 10000
        labels_list = []

        for i in range(0, N, chunk_size):
            X_chunk = X_tensor[i:i + chunk_size]
            diff = X_chunk.unsqueeze(1) - loc_mean.unsqueeze(0)
            log_probs = -0.5 * ((diff / scales_mean.unsqueeze(0)) ** 2).sum(dim=2)
            log_probs = log_probs - D * torch.log(scales_mean).sum(dim=1).unsqueeze(0)
            log_posterior = log_probs + log_weights.unsqueeze(0)
            chunk_labels = log_posterior.argmax(dim=1)
            labels_list.append(chunk_labels.cpu().numpy())

        labels = np.concatenate(labels_list)
        unique_labels = np.unique(labels)
        k_inferred = len(unique_labels)

        weights_np = weights.cpu().numpy()
        active_weights = weights_np[weights_np > 0.01]

    return ClusteringResult(
        labels=labels,
        k_inferred=k_inferred,
        elapsed_time=elapsed,
        method="PYP (Pyro/GPU)",
        extra_info={
            'discount': discount,
            'concentration': concentration,
            'n_steps': n_steps,
            'learning_rate': learning_rate,
            'final_loss': losses[-1] if losses else None,
            'n_active_components': len(active_weights),
            'weight_distribution': sorted(weights_np[weights_np > 0.001], reverse=True)[:10],
            'kmeans_init': use_kmeans_init
        }
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EVALUATION METRICS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class EvaluationResult:
    """Container for evaluation metrics."""
    nmi: float
    ari: float
    k_error: int
    k_ratio: float
    silhouette: Optional[float]

    def to_dict(self) -> Dict[str, float]:
        return {
            'NMI': self.nmi,
            'ARI': self.ari,
            'K_error': self.k_error,
            'K_ratio': self.k_ratio,
            'Silhouette': self.silhouette
        }


def evaluate_clustering(
    X: np.ndarray,
    y_true: np.ndarray,
    y_pred: np.ndarray,
    k_true: int,
    k_pred: int,
    compute_silhouette: bool = True
) -> EvaluationResult:
    """Compute clustering evaluation metrics."""
    nmi = normalized_mutual_info_score(y_true, y_pred)
    ari = adjusted_rand_score(y_true, y_pred)
    k_error = abs(k_pred - k_true)
    k_ratio = k_pred / k_true

    silhouette = None
    if compute_silhouette and len(X) <= 50000:
        try:
            if len(X) > 10000:
                idx = np.random.choice(len(X), 10000, replace=False)
                silhouette = silhouette_score(X[idx], y_pred[idx])
            else:
                silhouette = silhouette_score(X, y_pred)
        except Exception:
            pass

    return EvaluationResult(
        nmi=nmi,
        ari=ari,
        k_error=k_error,
        k_ratio=k_ratio,
        silhouette=silhouette
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN EXPERIMENT RUNNER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def run_single_comparison(
    n_samples: int = 50000,
    n_features: int = 60,
    n_true_clusters: int = 30,
    zipf_exponent: float = 2.0,  # More niche clusters
    cluster_separation: float = 0.7,  # Harder problem
    elliptical: bool = True,  # Elliptical clusters
    max_components: int = 100,
    pyp_discount: float = 0.25,
    concentration: float = 1.0,
    covariance_type: str = 'full',  # Full covariance for DP
    seed: int = 42,
    verbose: bool = True,
    dp_only: bool = False,  # Run only DP (CPU)
    pyp_only: bool = False,  # Run only PYP (GPU)
) -> Dict[str, Any]:
    """
    Run a single head-to-head comparison of DP vs PYP.
    """
    if verbose:
        print("=" * 70)
        print(f"EXPERIMENT: n={n_samples}, d={n_features}, K={n_true_clusters}")
        print(f"           zipf={zipf_exponent}, separation={cluster_separation}, elliptical={elliptical}")
        print("=" * 70)

    # Generate data
    if verbose:
        print("\n[1/4] Generating synthetic power-law data (HARD MODE)...")
    dataset = generate_power_law_clusters(
        n_samples=n_samples,
        n_features=n_features,
        zipf_exponent=zipf_exponent,
        n_true_clusters=n_true_clusters,
        cluster_separation=cluster_separation,
        elliptical=elliptical,
        seed=seed
    )
    if verbose:
        print(f"      Cluster sizes: {dataset.size_distribution_summary()}")

    # Fit DP (skip if pyp_only)
    result_dp = None
    eval_dp = None
    if not pyp_only:
        if verbose:
            print(f"\n[2/4] Fitting Dirichlet Process (sklearn, cov={covariance_type})...")
        result_dp = fit_dirichlet_process(
            dataset.X,
            max_components=max_components,
            concentration=concentration,
            covariance_type=covariance_type
        )
        eval_dp = evaluate_clustering(
            dataset.X, dataset.y_true, result_dp.labels,
            n_true_clusters, result_dp.k_inferred
        )
        if verbose:
            print(f"      K_inferred={result_dp.k_inferred}, NMI={eval_dp.nmi:.4f}, "
                  f"ARI={eval_dp.ari:.4f}, Time={result_dp.elapsed_time:.1f}s")
    else:
        if verbose:
            print("\n[2/4] Skipping DP (--pyp_only mode)")

    # Fit PYP (skip if dp_only)
    if dp_only:
        if verbose:
            print("\n[3/4] Skipping PYP (--dp_only mode)")
        result_pyp = None
        eval_pyp = None
    elif PYRO_AVAILABLE:
        if verbose:
            print(f"\n[3/4] Fitting Pitman-Yor Process (Pyro on {GPU_NAME}, TUNED)...")
        result_pyp = fit_pitman_yor_gpu(
            dataset.X,
            max_components=max_components,
            discount=pyp_discount,
            concentration=concentration,
            n_steps=2000,  # Tuned
            learning_rate=0.005,  # Tuned
            use_kmeans_init=True,  # K-means init
            verbose=verbose
        )
        eval_pyp = evaluate_clustering(
            dataset.X, dataset.y_true, result_pyp.labels,
            n_true_clusters, result_pyp.k_inferred
        )
        if verbose:
            print(f"      K_inferred={result_pyp.k_inferred}, NMI={eval_pyp.nmi:.4f}, "
                  f"ARI={eval_pyp.ari:.4f}, Time={result_pyp.elapsed_time:.1f}s")
    else:
        result_pyp = None
        eval_pyp = None
        if verbose:
            print("\n[3/4] Skipping PYP (Pyro not available)")

    # Summary
    if verbose:
        print("\n[4/4] Summary")
        print("-" * 50)
        print(f"True K: {n_true_clusters}")
        if result_dp:
            print(f"DP  inferred K: {result_dp.k_inferred} (error: {eval_dp.k_error})")
        if eval_pyp:
            print(f"PYP inferred K: {result_pyp.k_inferred} (error: {eval_pyp.k_error})")
            if eval_dp:
                print(f"\nNMI: DP={eval_dp.nmi:.4f}, PYP={eval_pyp.nmi:.4f}")
                if eval_dp.nmi > 0:
                    nmi_improvement = (eval_pyp.nmi - eval_dp.nmi) / eval_dp.nmi * 100
                    print(f"NMI improvement: {nmi_improvement:+.1f}%")
                print(f"\nARI: DP={eval_dp.ari:.4f}, PYP={eval_pyp.ari:.4f}")
                if eval_dp.ari > 0:
                    ari_improvement = (eval_pyp.ari - eval_dp.ari) / (eval_dp.ari + 1e-10) * 100
                    print(f"ARI improvement: {ari_improvement:+.1f}%")
            else:
                print(f"\nPYP NMI: {eval_pyp.nmi:.4f}, ARI: {eval_pyp.ari:.4f}")

    return {
        'dataset': {
            'n_samples': n_samples,
            'n_features': n_features,
            'n_true_clusters': n_true_clusters,
            'zipf_exponent': zipf_exponent,
            'cluster_separation': cluster_separation,
            'elliptical': elliptical,
            'cluster_sizes': dataset.cluster_sizes.tolist()
        },
        'dp': {
            'k_inferred': result_dp.k_inferred if result_dp else None,
            'time': result_dp.elapsed_time if result_dp else None,
            **(eval_dp.to_dict() if eval_dp else {})
        } if result_dp else None,
        'pyp': {
            'k_inferred': result_pyp.k_inferred if result_pyp else None,
            'time': result_pyp.elapsed_time if result_pyp else None,
            **(eval_pyp.to_dict() if eval_pyp else {})
        } if result_pyp else None
    }


def run_separation_sweep(
    separations: List[float] = [0.5, 0.7, 1.0, 1.5, 2.0],
    n_samples: int = 50000,
    n_features: int = 60,
    n_true_clusters: int = 30,
    zipf_exponent: float = 2.0,
    elliptical: bool = True,
    seed: int = 42,
    verbose: bool = True,
    dp_only: bool = False,
    pyp_only: bool = False,
) -> pd.DataFrame:
    """
    Find the crossover point where PYP beats DP.

    Tests across different cluster separations to find where
    PYP's power-law handling gives it an advantage.
    """
    results = []

    for sep in separations:
        if verbose:
            print(f"\n{'='*70}")
            print(f"SEPARATION SWEEP: separation={sep}")
            print('='*70)

        result = run_single_comparison(
            n_samples=n_samples,
            n_features=n_features,
            n_true_clusters=n_true_clusters,
            zipf_exponent=zipf_exponent,
            cluster_separation=sep,
            elliptical=elliptical,
            seed=seed,
            verbose=verbose,
            dp_only=dp_only,
            pyp_only=pyp_only,
        )

        row = {
            'separation': sep,
            'zipf_exp': zipf_exponent,
            'n_samples': n_samples,
            'elliptical': elliptical,
            'k_true': n_true_clusters,
            'k_dp': result['dp']['k_inferred'] if result['dp'] else None,
            'k_pyp': result['pyp']['k_inferred'] if result['pyp'] else None,
            'nmi_dp': result['dp']['NMI'] if result['dp'] else None,
            'nmi_pyp': result['pyp']['NMI'] if result['pyp'] else None,
            'ari_dp': result['dp']['ARI'] if result['dp'] else None,
            'ari_pyp': result['pyp']['ARI'] if result['pyp'] else None,
            'time_dp': result['dp']['time'] if result['dp'] else None,
            'time_pyp': result['pyp']['time'] if result['pyp'] else None,
        }

        if result['pyp'] and result['dp'] and result['pyp']['NMI'] and result['dp']['NMI'] > 0:
            row['nmi_improvement'] = (row['nmi_pyp'] - row['nmi_dp']) / row['nmi_dp'] * 100
            row['ari_improvement'] = (row['ari_pyp'] - row['ari_dp']) / (row['ari_dp'] + 1e-10) * 100
            row['k_error_dp'] = abs(row['k_dp'] - n_true_clusters)
            row['k_error_pyp'] = abs(row['k_pyp'] - n_true_clusters)
            row['pyp_wins'] = row['nmi_pyp'] > row['nmi_dp']

        results.append(row)

    df = pd.DataFrame(results)

    # Print summary
    if verbose:
        print("\n" + "=" * 70)
        print("SEPARATION SWEEP SUMMARY")
        print("=" * 70)

        # Check what data we have
        has_dp = df['nmi_dp'].notna().any()
        has_pyp = df['nmi_pyp'].notna().any()

        if has_dp and has_pyp:
            print("\n| Separation | DP NMI | PYP NMI | Winner | NMI Œî |")
            print("|------------|--------|---------|--------|-------|")
            for _, row in df.iterrows():
                winner = "PYP" if row.get('pyp_wins', False) else "DP"
                delta = row.get('nmi_improvement', 0) or 0
                print(f"| {row['separation']:.1f}        | {row['nmi_dp']:.4f} | {row['nmi_pyp']:.4f}  | {winner}    | {delta:+.1f}% |")
        elif has_dp:
            print("\n| Separation | DP K | DP NMI | DP ARI | Time (s) |")
            print("|------------|------|--------|--------|----------|")
            for _, row in df.iterrows():
                print(f"| {row['separation']:.1f}        | {row['k_dp']}   | {row['nmi_dp']:.4f} | {row['ari_dp']:.4f} | {row['time_dp']:.0f}     |")
            print("\n(DP only mode - run --pyp_only on GPU for comparison)")
        elif has_pyp:
            print("\n| Separation | PYP K | PYP NMI | PYP ARI | Time (s) |")
            print("|------------|-------|---------|---------|----------|")
            for _, row in df.iterrows():
                print(f"| {row['separation']:.1f}        | {row['k_pyp']}    | {row['nmi_pyp']:.4f}  | {row['ari_pyp']:.4f}  | {row['time_pyp']:.0f}     |")
            print("\n(PYP only mode - merge with DP results for comparison)")

        # Find crossover point (only if both have data)
        if has_dp and has_pyp:
            crossover = None
            for _, row in df.iterrows():
                if row.get('pyp_wins', False):
                    crossover = row['separation']
                    break

            print("\n" + "=" * 70)
            if crossover:
                print(f"‚úÖ CROSSOVER FOUND: PYP beats DP at separation ‚â§ {crossover}")
                print("   PYP advantage confirmed for hard clustering problems")
            else:
                print("‚ùå NO CROSSOVER: DP wins at all tested separations")
                print("   PYP advantage not observed in this configuration")

    return df


def run_parameter_sweep(
    zipf_exponents: List[float] = [1.5, 2.0, 2.5],
    sample_sizes: List[int] = [10000, 50000],
    separations: List[float] = [0.5, 0.7, 1.0],
    n_features: int = 60,
    n_true_clusters: int = 30,
    elliptical: bool = True,
    seeds: List[int] = [42],
    verbose: bool = True
) -> pd.DataFrame:
    """Run full parameter sweep."""
    results = []

    total = len(zipf_exponents) * len(sample_sizes) * len(separations) * len(seeds)
    exp_num = 0

    for zipf_exp in zipf_exponents:
        for n in sample_sizes:
            for sep in separations:
                for seed in seeds:
                    exp_num += 1
                    if verbose:
                        print(f"\n{'='*70}")
                        print(f"EXPERIMENT {exp_num}/{total}")
                        print(f"zipf={zipf_exp}, n={n}, separation={sep}")
                        print('='*70)

                    result = run_single_comparison(
                        n_samples=n,
                        n_features=n_features,
                        n_true_clusters=n_true_clusters,
                        zipf_exponent=zipf_exp,
                        cluster_separation=sep,
                        elliptical=elliptical,
                        seed=seed,
                        verbose=verbose
                    )

                    row = {
                        'zipf_exp': zipf_exp,
                        'n_samples': n,
                        'separation': sep,
                        'seed': seed,
                        'k_true': n_true_clusters,
                        'k_dp': result['dp']['k_inferred'],
                        'k_pyp': result['pyp']['k_inferred'] if result['pyp'] else None,
                        'nmi_dp': result['dp']['NMI'],
                        'nmi_pyp': result['pyp']['NMI'] if result['pyp'] else None,
                        'ari_dp': result['dp']['ARI'],
                        'ari_pyp': result['pyp']['ARI'] if result['pyp'] else None,
                        'time_dp': result['dp']['time'],
                        'time_pyp': result['pyp']['time'] if result['pyp'] else None,
                    }

                    if result['pyp'] and result['pyp']['NMI']:
                        if result['dp']['NMI'] > 0:
                            row['nmi_improvement'] = (row['nmi_pyp'] - row['nmi_dp']) / row['nmi_dp'] * 100
                        row['k_error_dp'] = abs(row['k_dp'] - n_true_clusters)
                        row['k_error_pyp'] = abs(row['k_pyp'] - n_true_clusters)

                    results.append(row)

    return pd.DataFrame(results)


def print_sweep_summary(df: pd.DataFrame):
    """Print summary statistics from parameter sweep."""
    print("\n" + "=" * 70)
    print("PARAMETER SWEEP SUMMARY")
    print("=" * 70)

    print("\nüìä OVERALL AVERAGES")
    print("-" * 50)
    print(f"Mean NMI - DP:  {df['nmi_dp'].mean():.4f} ¬± {df['nmi_dp'].std():.4f}")
    if 'nmi_pyp' in df.columns and df['nmi_pyp'].notna().any():
        print(f"Mean NMI - PYP: {df['nmi_pyp'].mean():.4f} ¬± {df['nmi_pyp'].std():.4f}")
        if 'nmi_improvement' in df.columns:
            print(f"Mean NMI improvement: {df['nmi_improvement'].mean():+.1f}%")

    print(f"\nMean K - True:  {df['k_true'].iloc[0]}")
    print(f"Mean K - DP:    {df['k_dp'].mean():.1f} ¬± {df['k_dp'].std():.1f}")
    if 'k_pyp' in df.columns and df['k_pyp'].notna().any():
        print(f"Mean K - PYP:   {df['k_pyp'].mean():.1f} ¬± {df['k_pyp'].std():.1f}")

    # By separation
    if 'separation' in df.columns and 'nmi_improvement' in df.columns:
        print("\nüìà BY CLUSTER SEPARATION (lower = harder)")
        print("-" * 50)
        by_sep = df.groupby('separation').agg({
            'nmi_dp': 'mean',
            'nmi_pyp': 'mean',
            'nmi_improvement': 'mean',
            'k_dp': 'mean',
            'k_pyp': 'mean'
        }).round(4)
        print(by_sep.to_string())

    # Validation verdict
    print("\n" + "=" * 70)
    print("üîç VALIDATION VERDICT")
    print("=" * 70)

    if 'nmi_improvement' in df.columns and df['nmi_improvement'].notna().any():
        mean_improvement = df['nmi_improvement'].mean()

        if mean_improvement >= 10:
            print(f"‚úÖ REPORTS VALIDATED: PYP shows {mean_improvement:.1f}% average NMI improvement")
            print("   Recommendation: Use PYP for HIMARI clustering")
        elif mean_improvement >= 5:
            print(f"‚ö†Ô∏è  PARTIAL VALIDATION: PYP shows {mean_improvement:.1f}% improvement")
            print("   Recommendation: PYP offers modest gains, weigh against complexity")
        else:
            print(f"‚ùå REPORTS NOT VALIDATED: Only {mean_improvement:.1f}% improvement observed")
            print("   Recommendation: Stick with DP (simpler, similar performance)")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLI INTERFACE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    parser = argparse.ArgumentParser(
        description='Validate Pitman-Yor vs Dirichlet Process clustering on power-law data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single comparison (HARD MODE)
  python validate_pyp_vs_dp.py --n_samples 50000 --separation 0.7

  # Separation sweep to find crossover point
  python validate_pyp_vs_dp.py --separation_sweep

  # Full parameter sweep
  python validate_pyp_vs_dp.py --run_sweep --output results.csv
        """
    )

    parser.add_argument('--n_samples', type=int, default=50000,
                        help='Number of samples (default: 50000)')
    parser.add_argument('--n_features', type=int, default=60,
                        help='Number of features (default: 60)')
    parser.add_argument('--n_clusters', type=int, default=30,
                        help='True number of clusters (default: 30)')
    parser.add_argument('--zipf_exponent', type=float, default=2.0,
                        help='Zipf exponent (default: 2.0, more niche clusters)')
    parser.add_argument('--separation', type=float, default=0.7,
                        help='Cluster separation (default: 0.7, hard problem)')
    parser.add_argument('--spherical', action='store_true',
                        help='Use spherical clusters (default: elliptical)')
    parser.add_argument('--discount', type=float, default=0.25,
                        help='PYP discount parameter (default: 0.25)')
    parser.add_argument('--concentration', type=float, default=1.0,
                        help='Concentration parameter (default: 1.0)')
    parser.add_argument('--max_components', type=int, default=100,
                        help='Maximum components (default: 100)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed (default: 42)')
    parser.add_argument('--separation_sweep', action='store_true',
                        help='Run separation sweep to find crossover')
    parser.add_argument('--run_sweep', action='store_true',
                        help='Run full parameter sweep')
    parser.add_argument('--quick', action='store_true',
                        help='Quick test with fewer configurations')
    parser.add_argument('--output', type=str, default=None,
                        help='Output CSV file')
    parser.add_argument('--quiet', action='store_true',
                        help='Minimal output')
    parser.add_argument('--dp_only', action='store_true',
                        help='Run only DP (sklearn, CPU) - for local machine')
    parser.add_argument('--pyp_only', action='store_true',
                        help='Run only PYP (Pyro, GPU) - for cloud GPU')

    args = parser.parse_args()

    # Print system info
    print("=" * 70)
    print("PYP vs DP CLUSTERING VALIDATION (HARD MODE)")
    print("=" * 70)
    print(f"PyTorch: {torch.__version__ if PYRO_AVAILABLE else 'N/A'}")
    print(f"Pyro:    {pyro.__version__ if PYRO_AVAILABLE else 'N/A'}")
    print(f"Device:  {GPU_NAME}")
    if GPU_AVAILABLE:
        print(f"GPU Memory: {GPU_MEMORY:.1f} GB")
    print("=" * 70)

    elliptical = not args.spherical

    if args.separation_sweep:
        # Separation sweep
        if args.quick:
            separations = [0.5, 1.0, 2.0]
        else:
            separations = [0.5, 0.7, 1.0, 1.5, 2.0]

        df = run_separation_sweep(
            separations=separations,
            n_samples=args.n_samples,
            n_features=args.n_features,
            n_true_clusters=args.n_clusters,
            zipf_exponent=args.zipf_exponent,
            elliptical=elliptical,
            seed=args.seed,
            verbose=not args.quiet,
            dp_only=args.dp_only,
            pyp_only=args.pyp_only,
        )

        if args.output:
            df.to_csv(args.output, index=False)
            print(f"\nResults saved to: {args.output}")

    elif args.run_sweep:
        # Full parameter sweep
        if args.quick:
            zipf_exponents = [1.5, 2.5]
            sample_sizes = [10000, 50000]
            separations = [0.5, 1.0]
        else:
            zipf_exponents = [1.5, 2.0, 2.5]
            sample_sizes = [10000, 50000]
            separations = [0.5, 0.7, 1.0]

        df = run_parameter_sweep(
            zipf_exponents=zipf_exponents,
            sample_sizes=sample_sizes,
            separations=separations,
            n_features=args.n_features,
            n_true_clusters=args.n_clusters,
            elliptical=elliptical,
            verbose=not args.quiet
        )

        if args.output:
            df.to_csv(args.output, index=False)
            print(f"\nResults saved to: {args.output}")

        print_sweep_summary(df)

        print("\nüìã FULL RESULTS TABLE")
        print("-" * 70)
        cols = ['separation', 'zipf_exp', 'n_samples', 'k_dp', 'k_pyp',
                'nmi_dp', 'nmi_pyp', 'nmi_improvement']
        cols = [c for c in cols if c in df.columns]
        print(df[cols].to_string(index=False))

    else:
        # Single comparison
        result = run_single_comparison(
            n_samples=args.n_samples,
            n_features=args.n_features,
            n_true_clusters=args.n_clusters,
            zipf_exponent=args.zipf_exponent,
            cluster_separation=args.separation,
            elliptical=elliptical,
            max_components=args.max_components,
            pyp_discount=args.discount,
            concentration=args.concentration,
            seed=args.seed,
            verbose=not args.quiet
        )

        if result['pyp'] and result['pyp']['NMI']:
            dp_nmi = result['dp']['NMI']
            pyp_nmi = result['pyp']['NMI']

            print("\n" + "=" * 70)
            print("üîç CONCLUSION")
            print("=" * 70)

            if dp_nmi > 0:
                nmi_improvement = (pyp_nmi - dp_nmi) / dp_nmi * 100

                if nmi_improvement >= 10:
                    print(f"‚úÖ PYP shows significant improvement ({nmi_improvement:.1f}%)")
                    print("   Reports VALIDATED for this configuration")
                elif nmi_improvement >= 0:
                    print(f"‚ö†Ô∏è  PYP shows modest improvement ({nmi_improvement:.1f}%)")
                else:
                    print(f"‚ùå DP outperforms PYP ({nmi_improvement:.1f}%)")
                    print("   Reports NOT validated for this configuration")
            else:
                print(f"PYP NMI: {pyp_nmi:.4f}, DP NMI: {dp_nmi:.4f}")


if __name__ == '__main__':
    main()
